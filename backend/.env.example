# CareerForge AI - Environment Variables
# Copy to .env and configure for your setup

# ============================================================
# LLM SERVICE CONFIGURATION
# ============================================================

# Ollama API endpoint (local or remote)
LLM_BASE_URL=http://localhost:11434

# Primary model for all AI tasks
# Options:
#   - mistral:7b-instruct-v0.3-q4_K_M (recommended, 4.1GB)
#   - mistral:7b-instruct (generic, 4.1GB)
#   - llama3:8b-instruct-q4_K_M (alternative, 4.7GB)
#   - mixtral:8x7b-instruct-v0.1-q4_K_M (powerful, 26GB)
LLM_MODEL=mistral:7b-instruct-v0.3-q4_K_M

# Fallback model if primary unavailable
LLM_FALLBACK_MODEL=mistral:7b-instruct

# Request timeout (seconds)
LLM_TIMEOUT=120

# Max retries on failure
LLM_MAX_RETRIES=3

# Temperature (0-1, lower = more deterministic)
LLM_TEMPERATURE=0.7

# Max tokens for response
LLM_MAX_TOKENS=4096


# ============================================================
# API SERVER CONFIGURATION
# ============================================================

# Server host (0.0.0.0 for Docker, localhost for dev)
API_HOST=0.0.0.0

# Server port
API_PORT=8000

# Enable debug mode (hot reload)
API_DEBUG=true

# CORS allowed origins (comma-separated)
# Use * for development, specific domains for production
CORS_ORIGINS=http://localhost:5173,http://localhost:3000

# Rate limiting (requests per minute per IP)
RATE_LIMIT=30


# ============================================================
# YOUTUBE SCRAPING
# ============================================================

# Enable YouTube video scraping
YOUTUBE_ENABLED=true

# Max videos per search
YOUTUBE_MAX_VIDEOS=3

# Minimum video duration in seconds (filters shorts)
YOUTUBE_MIN_DURATION=600


# ============================================================
# LOGGING
# ============================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO


# ============================================================
# LEGACY SUPPORT (for existing frontend)
# ============================================================
# Keep these empty to use open-source LLM instead

# Gemini API Keys (not used in v3)
GEMINI_API_KEYS=

# OpenAI API Keys (not used in v3)
OPENAI_API_KEYS=
OPENAI_MODEL=
