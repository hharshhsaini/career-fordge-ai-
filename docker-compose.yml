# CareerForge AI - Full Stack Docker Compose
# Runs frontend, backend, and LLM service together

version: "3.8"

services:
  # ============================================================
  # LLM SERVICE (Ollama)
  # ============================================================
  ollama:
    image: ollama/ollama:latest
    container_name: careerforge-llm
    restart: unless-stopped

    # GPU support - uncomment for NVIDIA GPUs
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

    ports:
      - "11434:11434"

    volumes:
      - ollama_data:/root/.ollama

    environment:
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_CONTEXT_SIZE=32768

    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:11434/api/tags" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ============================================================
  # BACKEND API (FastAPI)
  # ============================================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: careerforge-api
    restart: unless-stopped

    ports:
      - "8000:8000"

    environment:
      - LLM_BASE_URL=http://ollama:11434
      - LLM_MODEL=mistral:7b-instruct
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - API_DEBUG=false
      - CORS_ORIGINS=http://localhost:3000,http://localhost:5173

    depends_on:
      ollama:
        condition: service_healthy

    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/api/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============================================================
  # FRONTEND (React/Vite)
  # ============================================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: careerforge-web
    restart: unless-stopped

    ports:
      - "3000:80"

    environment:
      - VITE_API_URL=http://localhost:8000

    depends_on:
      - backend

  # ============================================================
  # MODEL LOADER (Initial setup)
  # ============================================================
  model-loader:
    image: curlimages/curl:latest
    container_name: careerforge-model-loader
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - |
        echo "Pulling Mistral model..."
        curl -X POST http://ollama:11434/api/pull \
          -d '{"name":"mistral:7b-instruct-v0.3-q4_K_M"}' \
          --max-time 600
        echo "Model loaded!"
    restart: "no"

volumes:
  ollama_data:
    driver: local
